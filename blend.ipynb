{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNa9-V2DuPXG"
      },
      "source": [
        "# Network Blending Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Google Drive Connection"
      ],
      "metadata": {
        "id": "zNUlrgLAzMp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU connection\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReFjlT7CvqrF",
        "outputId": "1e3fba50-5bc3-4946-efa8-31aa2f0d8713"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-7ce30f3f-266a-4b28-7cf3-fbc0f4bd9589)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXr8jWKVvq-E",
        "outputId": "482c3af6-4620-4547-ef36-1a9b378eeca4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!pip install gdown --upgrade\n",
        "\n",
        "if os.path.isdir(\"/content/drive/MyDrive/stylegan3-fun-blend\"):\n",
        "    %cd \"/content/drive/MyDrive/stylegan3-fun-blend\"\n",
        "elif os.path.isdir(\"/content/drive/\"):\n",
        "    #install script\n",
        "    %cd \"/content/drive/MyDrive/\"\n",
        "    !git clone https://github.com/adamdavidcole/stylegan3-fun-blend.git\n",
        "    %cd stylegan3-fun-blend\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets\n",
        "    !mkdir pretrained\n",
        "    # !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU -O /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/wikiart.pkl\n",
        "else:\n",
        "    !git clone https://github.com/adamdavidcole/stylegan3-fun-blend.git\n",
        "    %cd stylegan3-fun-blend\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets\n",
        "    !mkdir pretrained"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMpqnKu7v5og",
        "outputId": "67f9b469-edcd-410d-d32d-fee8ac2dd681"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.5.18.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/content/drive/MyDrive/stylegan3-fun-blend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update all code files in drive repo \n",
        "!git config --global user.name \"test\"\n",
        "!git config --global user.email \"test@test.com\"\n",
        "!git fetch origin\n",
        "!git pull\n",
        "!git stash\n",
        "!git checkout origin/main -- \"*.py\" \n",
        "!git checkout origin/main -- \"*.ipynb\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2iq2jrmxUAY",
        "outputId": "59b3a38a-d186-4635-dd9e-a8114a3d4a14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n",
            "Saved working directory and index state WIP on main: 59cff72 drive+collab test3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops ninja gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dKoUIUz86ac",
        "outputId": "e16278bb-4123-40b1-d581-04003e19596b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 22.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.7.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: ninja, einops\n",
            "Successfully installed einops-0.4.1 ninja-1.10.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "58xfSUxE66_W"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Setup"
      ],
      "metadata": {
        "id": "P5NZDdiT8FNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network_key = \"butterfly\"\n",
        "cfg=\"stylegan3-t\"\n",
        "pretrained_network = \"ffhqu256\"\n",
        "dataset = \"/content/drive/MyDrive/stylegan3/datasets/butterflys_256_2-256x256.zip\"\n",
        "\n",
        "results_outdir = f\"./results/{network_key}\"\n",
        "\n",
        "if not os.path.isdir(outdir):\n",
        "    !mkdir -p $outdir"
      ],
      "metadata": {
        "id": "33igFzDL8Hp5"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "dsoohbv60UPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --outdir=$outdir --cfg=$cfg --data=$dataset \\\n",
        "    --gpus=1 --batch=32 --batch-gpu=16  --gamma=6.6 --mirror=1 --kimg=50 --snap=1 --tick=1 --img-snap=1 --cbase=16384 \\\n",
        "    --resume=ffhqu256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oN1Lw-R9GBT",
        "outputId": "88994e69-7f04-48ab-82be-7afc29f17af1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating output directory...\n",
            "\n",
            "Training options:\n",
            "{\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks_stylegan3.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 2,\n",
            "      \"freeze_layers\": 0,\n",
            "      \"freeze_embed\": false\n",
            "    },\n",
            "    \"channel_base\": 32768,\n",
            "    \"channel_max\": 1024,\n",
            "    \"magnitude_ema_beta\": 0.9988915792636801,\n",
            "    \"conv_kernel\": 1,\n",
            "    \"use_radial_filters\": true\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks_stylegan2.Discriminator\",\n",
            "    \"block_kwargs\": {\n",
            "      \"freeze_layers\": 0\n",
            "    },\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08,\n",
            "    \"lr\": 0.0025\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08,\n",
            "    \"lr\": 0.002\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 6.6,\n",
            "    \"blur_init_sigma\": 0,\n",
            "    \"blur_fade_kimg\": 200.0\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"prefetch_factor\": 2,\n",
            "    \"num_workers\": 3\n",
            "  },\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"/content/drive/MyDrive/stylegan3/datasets/butterflys_256_2-256x256.zip\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 10051,\n",
            "    \"xflip\": true,\n",
            "    \"yflip\": false,\n",
            "    \"resolution\": 256,\n",
            "    \"random_seed\": 0\n",
            "  },\n",
            "  \"num_gpus\": 1,\n",
            "  \"batch_size\": 32,\n",
            "  \"batch_gpu\": 16,\n",
            "  \"metrics\": [],\n",
            "  \"total_kimg\": 50,\n",
            "  \"resume_kimg\": 0,\n",
            "  \"kimg_per_tick\": 1,\n",
            "  \"network_snapshot_ticks\": 1,\n",
            "  \"image_snapshot_ticks\": 1,\n",
            "  \"snap_res\": \"4k\",\n",
            "  \"random_seed\": 0,\n",
            "  \"ema_kimg\": 10.0,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1,\n",
            "    \"brightness\": 1,\n",
            "    \"contrast\": 1,\n",
            "    \"lumaflip\": 1,\n",
            "    \"hue\": 1,\n",
            "    \"saturation\": 1\n",
            "  },\n",
            "  \"ada_target\": 0.6,\n",
            "  \"resume_pkl\": \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-256x256.pkl\",\n",
            "  \"ada_kimg\": 100,\n",
            "  \"ema_rampup\": null,\n",
            "  \"run_dir\": \"./results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256\"\n",
            "}\n",
            "\n",
            "Output directory:    ./results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256\n",
            "Number of GPUs:      1\n",
            "Batch size:          32 images\n",
            "Training duration:   50 kimg\n",
            "Dataset path:        /content/drive/MyDrive/stylegan3/datasets/butterflys_256_2-256x256.zip\n",
            "Dataset size:        10051 images\n",
            "Dataset resolution:  256\n",
            "Dataset labels:      False\n",
            "Dataset x-flips:     True\n",
            "Dataset y-flips:     False\n",
            "\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "\n",
            "Num images:  20102\n",
            "Image shape: [3, 256, 256]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Resuming from \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-256x256.pkl\"\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"filtered_lrelu_plugin\"... Done.\n",
            "\n",
            "Generator                     Parameters  Buffers  Output shape         Datatype\n",
            "---                           ---         ---      ---                  ---     \n",
            "mapping.fc0                   262656      -        [16, 512]            float32 \n",
            "mapping.fc1                   262656      -        [16, 512]            float32 \n",
            "mapping                       -           512      [16, 16, 512]        float32 \n",
            "synthesis.input.affine        2052        -        [16, 4]              float32 \n",
            "synthesis.input               1048576     3081     [16, 1024, 36, 36]   float32 \n",
            "synthesis.L0_36_1024.affine   525312      -        [16, 1024]           float32 \n",
            "synthesis.L0_36_1024          1049600     157      [16, 1024, 36, 36]   float32 \n",
            "synthesis.L1_36_1024.affine   525312      -        [16, 1024]           float32 \n",
            "synthesis.L1_36_1024          1049600     157      [16, 1024, 36, 36]   float32 \n",
            "synthesis.L2_36_1024.affine   525312      -        [16, 1024]           float32 \n",
            "synthesis.L2_36_1024          1049600     157      [16, 1024, 36, 36]   float32 \n",
            "synthesis.L3_52_1024.affine   525312      -        [16, 1024]           float32 \n",
            "synthesis.L3_52_1024          1049600     169      [16, 1024, 52, 52]   float16 \n",
            "synthesis.L4_52_1024.affine   525312      -        [16, 1024]           float32 \n",
            "synthesis.L4_52_1024          1049600     157      [16, 1024, 52, 52]   float16 \n",
            "synthesis.L5_84_1024.affine   525312      -        [16, 1024]           float32 \n",
            "synthesis.L5_84_1024          1049600     169      [16, 1024, 84, 84]   float16 \n",
            "synthesis.L6_84_1024.affine   525312      -        [16, 1024]           float32 \n",
            "synthesis.L6_84_1024          1049600     157      [16, 1024, 84, 84]   float16 \n",
            "synthesis.L7_148_724.affine   525312      -        [16, 1024]           float32 \n",
            "synthesis.L7_148_724          742100      169      [16, 724, 148, 148]  float16 \n",
            "synthesis.L8_148_512.affine   371412      -        [16, 724]            float32 \n",
            "synthesis.L8_148_512          371200      157      [16, 512, 148, 148]  float16 \n",
            "synthesis.L9_148_362.affine   262656      -        [16, 512]            float32 \n",
            "synthesis.L9_148_362          185706      157      [16, 362, 148, 148]  float16 \n",
            "synthesis.L10_276_256.affine  185706      -        [16, 362]            float32 \n",
            "synthesis.L10_276_256         92928       169      [16, 256, 276, 276]  float16 \n",
            "synthesis.L11_276_181.affine  131328      -        [16, 256]            float32 \n",
            "synthesis.L11_276_181         46517       157      [16, 181, 276, 276]  float16 \n",
            "synthesis.L12_276_128.affine  92853       -        [16, 181]            float32 \n",
            "synthesis.L12_276_128         23296       25       [16, 128, 276, 276]  float16 \n",
            "synthesis.L13_256_128.affine  65664       -        [16, 128]            float32 \n",
            "synthesis.L13_256_128         16512       25       [16, 128, 256, 256]  float16 \n",
            "synthesis.L14_256_3.affine    65664       -        [16, 128]            float32 \n",
            "synthesis.L14_256_3           387         1        [16, 3, 256, 256]    float16 \n",
            "synthesis                     -           -        [16, 3, 256, 256]    float32 \n",
            "---                           ---         ---      ---                  ---     \n",
            "Total                         15779565    5576     -                    -       \n",
            "\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape         Datatype\n",
            "---            ---         ---      ---                  ---     \n",
            "b256.fromrgb   256         16       [16, 64, 256, 256]   float16 \n",
            "b256.skip      8192        16       [16, 128, 128, 128]  float16 \n",
            "b256.conv0     36928       16       [16, 64, 256, 256]   float16 \n",
            "b256.conv1     73856       16       [16, 128, 128, 128]  float16 \n",
            "b256           -           16       [16, 128, 128, 128]  float16 \n",
            "b128.skip      32768       16       [16, 256, 64, 64]    float16 \n",
            "b128.conv0     147584      16       [16, 128, 128, 128]  float16 \n",
            "b128.conv1     295168      16       [16, 256, 64, 64]    float16 \n",
            "b128           -           16       [16, 256, 64, 64]    float16 \n",
            "b64.skip       131072      16       [16, 512, 32, 32]    float16 \n",
            "b64.conv0      590080      16       [16, 256, 64, 64]    float16 \n",
            "b64.conv1      1180160     16       [16, 512, 32, 32]    float16 \n",
            "b64            -           16       [16, 512, 32, 32]    float16 \n",
            "b32.skip       262144      16       [16, 512, 16, 16]    float16 \n",
            "b32.conv0      2359808     16       [16, 512, 32, 32]    float16 \n",
            "b32.conv1      2359808     16       [16, 512, 16, 16]    float16 \n",
            "b32            -           16       [16, 512, 16, 16]    float16 \n",
            "b16.skip       262144      16       [16, 512, 8, 8]      float32 \n",
            "b16.conv0      2359808     16       [16, 512, 16, 16]    float32 \n",
            "b16.conv1      2359808     16       [16, 512, 8, 8]      float32 \n",
            "b16            -           16       [16, 512, 8, 8]      float32 \n",
            "b8.skip        262144      16       [16, 512, 4, 4]      float32 \n",
            "b8.conv0       2359808     16       [16, 512, 8, 8]      float32 \n",
            "b8.conv1       2359808     16       [16, 512, 4, 4]      float32 \n",
            "b8             -           16       [16, 512, 4, 4]      float32 \n",
            "b4.mbstd       -           -        [16, 513, 4, 4]      float32 \n",
            "b4.conv        2364416     16       [16, 512, 4, 4]      float32 \n",
            "b4.fc          4194816     -        [16, 512]            float32 \n",
            "b4.out         513         -        [16, 1]              float32 \n",
            "---            ---         ---      ---                  ---     \n",
            "Total          24001089    416      -                    -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "Training for 50 kimg...\n",
            "\n",
            "/content/drive/MyDrive/stylegan3-fun-blend/training/augment.py:231: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at  ../aten/src/ATen/native/cuda/jit_utils.cpp:860.)\n",
            "  s = torch.exp2(torch.randn([batch_size], device=device) * self.scale_std)\n",
            "tick 0     kimg 0.0      time 43s          sec/tick 17.3    sec/kimg 541.14  maintenance 25.6   cpumem 4.07   gpumem 10.46  reserved 12.97  augment 0.000\n",
            "tick 1     kimg 1.1      time 3m 44s       sec/tick 173.1   sec/kimg 169.04  maintenance 8.1    cpumem 4.36   gpumem 8.91   reserved 12.38  augment 0.006\n",
            "tick 2     kimg 2.1      time 6m 45s       sec/tick 173.2   sec/kimg 169.11  maintenance 8.1    cpumem 4.45   gpumem 8.93   reserved 12.38  augment 0.017\n",
            "tick 3     kimg 3.1      time 9m 47s       sec/tick 173.2   sec/kimg 169.13  maintenance 8.1    cpumem 4.45   gpumem 8.93   reserved 12.38  augment 0.024\n",
            "tick 4     kimg 4.1      time 12m 48s      sec/tick 173.2   sec/kimg 169.17  maintenance 8.1    cpumem 4.45   gpumem 8.93   reserved 12.38  augment 0.035\n",
            "tick 5     kimg 5.2      time 15m 49s      sec/tick 173.2   sec/kimg 169.15  maintenance 8.1    cpumem 4.45   gpumem 8.95   reserved 12.38  augment 0.042\n",
            "tick 6     kimg 6.2      time 18m 50s      sec/tick 173.2   sec/kimg 169.15  maintenance 8.1    cpumem 4.45   gpumem 8.93   reserved 12.38  augment 0.050\n",
            "tick 7     kimg 7.2      time 21m 52s      sec/tick 173.2   sec/kimg 169.17  maintenance 8.1    cpumem 4.45   gpumem 8.94   reserved 12.38  augment 0.058\n",
            "tick 8     kimg 8.2      time 24m 53s      sec/tick 173.2   sec/kimg 169.17  maintenance 8.1    cpumem 4.54   gpumem 8.94   reserved 12.38  augment 0.065\n",
            "tick 9     kimg 9.2      time 27m 54s      sec/tick 173.2   sec/kimg 169.18  maintenance 8.1    cpumem 4.54   gpumem 8.95   reserved 12.39  augment 0.073\n",
            "tick 10    kimg 10.3     time 30m 56s      sec/tick 173.2   sec/kimg 169.17  maintenance 8.0    cpumem 4.54   gpumem 8.95   reserved 12.39  augment 0.081\n",
            "tick 11    kimg 11.3     time 33m 57s      sec/tick 173.3   sec/kimg 169.19  maintenance 8.0    cpumem 4.54   gpumem 8.96   reserved 12.39  augment 0.086\n",
            "tick 12    kimg 12.3     time 36m 58s      sec/tick 173.2   sec/kimg 169.17  maintenance 8.0    cpumem 4.54   gpumem 8.95   reserved 12.39  augment 0.093\n",
            "tick 13    kimg 13.3     time 39m 59s      sec/tick 173.2   sec/kimg 169.18  maintenance 8.0    cpumem 4.54   gpumem 8.94   reserved 12.39  augment 0.101\n",
            "tick 14    kimg 14.4     time 43m 01s      sec/tick 173.3   sec/kimg 169.20  maintenance 8.0    cpumem 4.54   gpumem 9.01   reserved 12.39  augment 0.111\n",
            "tick 15    kimg 15.4     time 46m 02s      sec/tick 173.3   sec/kimg 169.20  maintenance 8.0    cpumem 4.54   gpumem 8.95   reserved 12.39  augment 0.116\n",
            "tick 16    kimg 16.4     time 49m 03s      sec/tick 173.3   sec/kimg 169.21  maintenance 8.0    cpumem 4.54   gpumem 8.97   reserved 12.39  augment 0.122\n",
            "tick 17    kimg 17.4     time 52m 05s      sec/tick 173.3   sec/kimg 169.22  maintenance 8.0    cpumem 4.54   gpumem 9.00   reserved 12.39  augment 0.132\n",
            "tick 18    kimg 18.5     time 55m 06s      sec/tick 173.3   sec/kimg 169.19  maintenance 8.0    cpumem 4.54   gpumem 8.99   reserved 12.39  augment 0.132\n",
            "tick 19    kimg 19.5     time 58m 07s      sec/tick 173.2   sec/kimg 169.19  maintenance 8.0    cpumem 4.54   gpumem 8.99   reserved 12.39  augment 0.140\n",
            "tick 20    kimg 20.5     time 1h 01m 08s   sec/tick 173.3   sec/kimg 169.22  maintenance 8.0    cpumem 4.54   gpumem 8.97   reserved 12.39  augment 0.150\n",
            "tick 21    kimg 21.5     time 1h 04m 10s   sec/tick 173.3   sec/kimg 169.23  maintenance 8.0    cpumem 4.54   gpumem 8.99   reserved 12.39  augment 0.157\n",
            "tick 22    kimg 22.6     time 1h 07m 11s   sec/tick 173.3   sec/kimg 169.25  maintenance 8.0    cpumem 4.54   gpumem 8.99   reserved 12.39  augment 0.165\n",
            "tick 23    kimg 23.6     time 1h 10m 12s   sec/tick 173.3   sec/kimg 169.23  maintenance 7.9    cpumem 4.54   gpumem 8.97   reserved 12.39  augment 0.172\n",
            "tick 24    kimg 24.6     time 1h 13m 13s   sec/tick 173.3   sec/kimg 169.24  maintenance 7.9    cpumem 4.54   gpumem 9.02   reserved 12.39  augment 0.179\n",
            "tick 25    kimg 25.6     time 1h 16m 14s   sec/tick 173.3   sec/kimg 169.26  maintenance 7.9    cpumem 4.54   gpumem 9.00   reserved 12.39  augment 0.184\n",
            "tick 26    kimg 26.7     time 1h 19m 15s   sec/tick 173.3   sec/kimg 169.24  maintenance 7.8    cpumem 4.54   gpumem 9.01   reserved 12.39  augment 0.187\n",
            "tick 27    kimg 27.7     time 1h 22m 17s   sec/tick 173.3   sec/kimg 169.25  maintenance 7.8    cpumem 4.54   gpumem 8.99   reserved 12.39  augment 0.192\n",
            "tick 28    kimg 28.7     time 1h 25m 18s   sec/tick 173.3   sec/kimg 169.23  maintenance 7.8    cpumem 4.54   gpumem 8.97   reserved 12.39  augment 0.195\n",
            "tick 29    kimg 29.7     time 1h 28m 19s   sec/tick 173.3   sec/kimg 169.26  maintenance 7.8    cpumem 4.54   gpumem 8.97   reserved 12.39  augment 0.200\n",
            "tick 30    kimg 30.8     time 1h 31m 20s   sec/tick 173.3   sec/kimg 169.25  maintenance 7.8    cpumem 4.54   gpumem 8.96   reserved 12.39  augment 0.202\n",
            "tick 31    kimg 31.8     time 1h 34m 21s   sec/tick 173.3   sec/kimg 169.29  maintenance 7.8    cpumem 4.54   gpumem 9.02   reserved 12.39  augment 0.205\n",
            "tick 32    kimg 32.8     time 1h 37m 22s   sec/tick 173.3   sec/kimg 169.28  maintenance 7.8    cpumem 4.54   gpumem 8.97   reserved 12.39  augment 0.207\n",
            "tick 33    kimg 33.8     time 1h 40m 23s   sec/tick 173.3   sec/kimg 169.27  maintenance 7.8    cpumem 4.54   gpumem 8.98   reserved 12.39  augment 0.207\n",
            "tick 34    kimg 34.8     time 1h 43m 25s   sec/tick 173.3   sec/kimg 169.24  maintenance 7.8    cpumem 4.54   gpumem 8.99   reserved 12.39  augment 0.206\n",
            "tick 35    kimg 35.9     time 1h 46m 26s   sec/tick 173.3   sec/kimg 169.26  maintenance 7.8    cpumem 4.54   gpumem 8.98   reserved 12.39  augment 0.209\n",
            "tick 36    kimg 36.9     time 1h 49m 27s   sec/tick 173.4   sec/kimg 169.29  maintenance 7.8    cpumem 4.54   gpumem 8.99   reserved 12.39  augment 0.211\n",
            "tick 37    kimg 37.9     time 1h 52m 28s   sec/tick 173.4   sec/kimg 169.29  maintenance 7.8    cpumem 4.54   gpumem 9.02   reserved 12.39  augment 0.211\n",
            "tick 38    kimg 38.9     time 1h 55m 29s   sec/tick 173.4   sec/kimg 169.33  maintenance 7.8    cpumem 4.54   gpumem 8.99   reserved 12.39  augment 0.214\n",
            "tick 39    kimg 40.0     time 1h 58m 30s   sec/tick 173.4   sec/kimg 169.32  maintenance 7.8    cpumem 4.54   gpumem 8.97   reserved 12.39  augment 0.214\n",
            "tick 40    kimg 41.0     time 2h 01m 32s   sec/tick 173.4   sec/kimg 169.32  maintenance 7.8    cpumem 4.54   gpumem 9.00   reserved 12.39  augment 0.221\n",
            "tick 41    kimg 42.0     time 2h 04m 33s   sec/tick 173.4   sec/kimg 169.32  maintenance 7.8    cpumem 4.54   gpumem 9.01   reserved 12.39  augment 0.221\n",
            "tick 42    kimg 43.0     time 2h 07m 34s   sec/tick 173.4   sec/kimg 169.30  maintenance 7.8    cpumem 4.54   gpumem 9.01   reserved 12.39  augment 0.224\n",
            "tick 43    kimg 44.1     time 2h 10m 35s   sec/tick 173.3   sec/kimg 169.25  maintenance 7.7    cpumem 4.54   gpumem 8.99   reserved 12.39  augment 0.227\n",
            "tick 44    kimg 45.1     time 2h 13m 36s   sec/tick 173.3   sec/kimg 169.26  maintenance 7.7    cpumem 4.54   gpumem 8.98   reserved 12.39  augment 0.224\n",
            "\n",
            "Aborted!\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Projection"
      ],
      "metadata": {
        "id": "CHXnE2IO7lkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network_source_folder = \"00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256\""
      ],
      "metadata": {
        "id": "YM7U4UlGuIF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload File \n",
        "outdir = \"projection_source_images\"\n",
        "if not os.path.isdir(outdir):\n",
        "  !mkdir -p $outdir\n",
        "\n",
        "def upload_files():\n",
        "  filepaths = []\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()\n",
        "  for k, v in uploaded.items():\n",
        "    filepath = f\"{outdir}/{k}\"\n",
        "    open(filepath, 'wb').write(v)\n",
        "    filepaths.append(filepath)\n",
        "  return list(filepaths)\n",
        "\n",
        "uploaded = upload_files();\n",
        "print(uploaded)"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "OfZ0Sc3EM97W",
        "outputId": "d914db2f-8eb4-49d1-bfdb-4048eff5ea24"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5176757a-a729-47fc-98a3-960104d80dff\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5176757a-a729-47fc-98a3-960104d80dff\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving self-image.jpeg to self-image (2).jpeg\n",
            "['projection_source_images/self-image.jpeg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Project Image\n",
        "uploaded_file = uploaded[0]\n",
        "print(uploaded_file)\n",
        "\n",
        "!python projector.py --target=$uploaded_file --project-in-wplus --save-video --num-steps=5000 --stabilize-projection \\\n",
        "       --cfg=stylegan3-r --network=ffhqu256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS78XvhaOzWN",
        "outputId": "71bbf992-7d5b-493e-8d73-64457e3b4102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "projection_source_images/self-image.jpeg\n",
            "Loading networks from \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-256x256.pkl\"...\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Projecting in W+ latent space...\n",
            "Starting from W midpoint using 10000 samples...\n",
            "Setting up PyTorch plugin \"filtered_lrelu_plugin\"... Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "5I7fzlAR7qfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "most_recent_training_result = os.listdir(results_outdir)[-1]\n",
        "print(most_recent_training_result)\n",
        "\n",
        "path_to_most_recent_training_result = f\"{results_outdir}/{most_recent_training_result}\"\n",
        "training_checkpoints = [f\"{path_to_most_recent_training_result}/{f}\" for f in os.listdir(path_to_most_recent_training_result) if f.endswith('.pkl')]\n",
        "print(training_checkpoints)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3qW_Mgh_X1L",
        "outputId": "121e516c-2662-4e17-99db-f71f07c8f619"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256\n",
            "['./results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000000.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000001.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000002.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000003.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000004.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000005.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000006.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000007.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000008.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000009.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000010.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000011.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000012.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000013.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000014.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000015.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000016.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000017.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000018.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000019.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000020.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000021.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000022.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000023.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000024.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000025.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000026.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000027.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000028.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000029.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000030.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000031.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000032.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000033.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000034.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000035.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000036.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000037.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000038.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000039.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000040.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000042.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000043.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000044.pkl', './results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000045.pkl']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "projected_w_path = \"out/projection/00004-projection-wplus-wavgstart-sgan2/projected_wplus_wavg_final.npy\"\n",
        "result_name=\"test_result\"\n",
        "# projection_network_pkl = gen_utils.resume_specs[\"stylegan3-r\"][\"ffhqu256\"];\n",
        "# projection_network_pkl = \n",
        "\n",
        "projection_outdir=f\"{outdir}/projections\"\n",
        "\n",
        "print(projection_outdir)\n",
        "\n",
        "if not os.path.isdir(projection_outdir):\n",
        "  !mkdir -p $projection_outdir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO6SoYj_6BN6",
        "outputId": "bb04d947-572e-4cf2-8482-d6ab70bed41c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "projection_source_images/projections\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate image from projection\n",
        "projected_w = np.load(projected_w_path)\n",
        "print(projected_w.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQJZ0q6OO1-8",
        "outputId": "a0c57b1a-ce4e-4036-a22e-075fd6b9d4a8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 16, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate N images across network blend\n",
        "import torch\n",
        "import dnnlib\n",
        "from dnnlib.util import format_time\n",
        "import legacy\n",
        "import PIL.Image\n",
        "\n",
        "from torch_utils import gen_utils\n",
        "\n",
        "\n",
        "# projection_network_pkl = gen_utils.resume_specs[\"stylegan3-r\"][\"ffhqu256\"]\n",
        "\n",
        "\n",
        "def gen_img_from_network(network_pkl_path):\n",
        "  network_pkl_name = network_pkl_path.split('/')[-1]\n",
        "  print('Loading networks from \"%s\"...' % network_pkl_path)\n",
        "  device = torch.device('cuda')\n",
        "  with dnnlib.util.open_url(network_pkl_path) as fp:\n",
        "      G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device)\n",
        "\n",
        "  projected_w_tensor = torch.tensor(projected_w).to(device)\n",
        "  synth_image = gen_utils.w_to_img(G, dlatents=projected_w_tensor, noise_mode='const')[0]\n",
        "  PIL.Image.fromarray(synth_image, 'RGB').save(f'{projection_outdir}/{result_name}_{network_pkl_name}.jpg')\n",
        "\n",
        "for i in range(10):\n",
        "  gen_img_from_network(training_checkpoints[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxiBj8ZuO59O",
        "outputId": "d704b3a5-f3cd-4951-9455-547bae40e372"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading networks from \"./results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000000.pkl\"...\n",
            "Loading networks from \"./results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000001.pkl\"...\n",
            "Loading networks from \"./results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000002.pkl\"...\n",
            "Loading networks from \"./results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000003.pkl\"...\n",
            "Loading networks from \"./results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000004.pkl\"...\n",
            "Loading networks from \"./results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000005.pkl\"...\n",
            "Loading networks from \"./results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000006.pkl\"...\n",
            "Loading networks from \"./results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000007.pkl\"...\n",
            "Loading networks from \"./results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000008.pkl\"...\n",
            "Loading networks from \"./results/butterfly/00005-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6-resume_ffhqu256/network-snapshot-000009.pkl\"...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selection"
      ],
      "metadata": {
        "id": "LHsHlJHI7viM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Refinement"
      ],
      "metadata": {
        "id": "WilXQgAn7_-x"
      }
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "a8b6964c4b11971ada34c31ca3858cac76f4677604ed4191f9e9f62d73997475"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('pytorch2')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "blend.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}